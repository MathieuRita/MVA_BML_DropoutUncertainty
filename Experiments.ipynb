{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\"> Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from net_bml import Net\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Choose a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the data between the following dataset:\n",
    "\n",
    "Regression problems:\n",
    "\n",
    "- `bostonHousing`\n",
    "- `concrete`\n",
    "- `energy`\n",
    "- `kin8mn`\n",
    "- `naval-propulsion-plant`\n",
    "- `power-plant`\n",
    "- `protein-tertiary-sctructure`\n",
    "- `wine-quality-red`\n",
    "- `yatch`\n",
    "\n",
    "Classification problems:\n",
    "\n",
    "- `MNIST`\n",
    "\n",
    "Then select the batch sizes (`batch_size_train`, `batch_size_test`) used for the next training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training set is:\n",
      "torch.Size([455, 13])\n",
      "The shape of the testing set is:\n",
      "torch.Size([51, 13])\n"
     ]
    }
   ],
   "source": [
    "data_name = \"bostonHousing\"\n",
    "batch_size_train= 128\n",
    "batch_size_test= 50\n",
    "\n",
    "training_set, testing_set, train_loader, test_loader, x_mean, x_std, y_mean, y_std = load_data(data_name=data_name,\n",
    "                                                                                                          batch_size_train=batch_size_train, \n",
    "                                                                                                          batch_size_test = batch_size_test,\n",
    "                                                                                                          normalize=True)\n",
    "\n",
    "print(\"The shape of the training set is:\")\n",
    "print(training_set.data.size())\n",
    "print(\"The shape of the testing set is:\")\n",
    "print(testing_set.data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Create the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's choose a network architecture. We only study here linear layers (ReLu activation) separated by a dropout layer. The weights $\\W_{i}$ and bias $b_{i}$ of the linear layers are regularized ($L_2$) with the regularization parameter $\\lambda$.\n",
    "\n",
    "You have to set the following parameters:\n",
    "- the number of input data $N$ (eg.`N=1000`)\n",
    "- the number of input features $n_{in}$ (eg. `n_in=784`)\n",
    "- the number of classes $n_{out}$ (eg. `n_out=10`)\n",
    "- number of layers and neurons (eg. `layer_sizes=[256,256,256]`)\n",
    "- the dropout rate $p$ (eg. `dropout_rate=0.5`)\n",
    "- $\\tau$ (eg. `tau=0.5`)\n",
    "\n",
    "Please note the regularization parameter $\\lambda$ is compute according to the following formula:\n",
    "\n",
    "\n",
    "<div align=\"center\"> $\\lambda=\\frac{pl^{2}}{2N\\tau}$\n",
    "    \n",
    "where $l$ is the prior-length scale. It is arbitrary set of `1e-2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (model): Sequential(\n",
      "    (Dropout0): Dropout(p=0.005, inplace=False)\n",
      "    (Linear0): Linear(in_features=13, out_features=100, bias=True)\n",
      "    (Relu): ReLU()\n",
      "    (Dropout1): Dropout(p=0.005, inplace=False)\n",
      "    (Linear1): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters\n",
    "N=training_set.data.size()[0]\n",
    "n_in=training_set.data.size()[1]\n",
    "n_out=1\n",
    "layer_sizes=[100,]\n",
    "dropout_rate= 0.005 \n",
    "tau=0.1\n",
    "l=1e-2\n",
    "lbda = l**2 * (1 - dropout_rate) / (2. * N * tau)\n",
    "\n",
    "# Create the network\n",
    "network = Net(N=N,\n",
    "              n_in=n_in,\n",
    "              n_out=n_out,\n",
    "              layer_sizes=layer_sizes,\n",
    "              dropout_rate=dropout_rate,\n",
    "              tau=tau).float()\n",
    "\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the useful functions to run the training.\n",
    "\n",
    "You have to choose the type of the problem, either classification or regression:\n",
    "- `problem=\"classification\"` or `problem=regression`\n",
    "\n",
    "It will define the loss to use, either NLL for classification or RMSE for regression.\n",
    "\n",
    "Then, you have to choose the parameters of the training:\n",
    "- number of epochs (eg. `n_epochs=10`)\n",
    "- learning rate (eg. `learning_rate=0.01`)\n",
    "- momentum (eg. `momentum=0.5`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of the problem\n",
    "problem=\"regression\"\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 300\n",
    "learning_rate = 1e-8\n",
    "momentum = 0.5\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), \n",
    "                      lr=learning_rate,\n",
    "                     weight_decay=lbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "def train(network,epoch,aff=False):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        if data_name==\"MNIST\":\n",
    "            data=data.squeeze(dim=1)\n",
    "            data=data.flatten(1,2)\n",
    "            \n",
    "        output = network(data)\n",
    "        \n",
    "        if problem==\"classification\":\n",
    "            loss = F.nll_loss(output, target)\n",
    "        if problem==\"regression\":\n",
    "            loss = F.mse_loss(output,target)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            if aff:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "\n",
    "def test(network,aff=False):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            if data_name==\"MNIST\":\n",
    "                data=data.squeeze(dim=1)\n",
    "                data=data.flatten(1,2)\n",
    "                \n",
    "            output = network(data)\n",
    "            \n",
    "            if problem==\"classification\":\n",
    "                test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            if problem==\"regression\":\n",
    "                test_loss += F.mse_loss(output,target,reduction=\"mean\").item()\n",
    "                \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    if aff:\n",
    "        if problem==\"classification\":\n",
    "            print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, correct, len(test_loader.dataset),\n",
    "                100. * correct / len(test_loader.dataset)))\n",
    "        if problem==\"regression\":\n",
    "            print('\\nTest set: Avg. loss: {:.4f}\\n'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's run the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Using a target size (torch.Size([71])) that is different to the input size (torch.Size([71, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([50, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(network,epoch)\n",
    "    test(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV - Perform the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.33067841214774\n",
      "21.833019147558502\n",
      "-25.904394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30.33067841214774, 21.833019147558502, -25.904394)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prediction(network,testing_set,normalize=True):\n",
    "\n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        standard_pred = network(testing_set.data)\n",
    "\n",
    "    rmse_standard_pred = np.mean((testing_set.targets.data.numpy() - standard_pred.squeeze(dim=1).data.numpy())**2.)**0.5\n",
    "    if normalize:\n",
    "        rmse_standard_pred*=y_std\n",
    "        rmse_standard_pred+=y_mean\n",
    "    \n",
    "    T = 10000\n",
    "\n",
    "    Yt_hat=[]\n",
    "    for _ in range(T):\n",
    "        Yt_hat.append(network(testing_set.data).data.numpy())\n",
    "        optimizer.step()\n",
    "    Yt_hat=np.array(Yt_hat)\n",
    "    if normalize:\n",
    "        Yt_hat = Yt_hat * y_std + y_mean\n",
    "    MC_pred = np.mean(Yt_hat, 0)\n",
    "    rmse = np.mean((testing_set.data.numpy() - MC_pred)**2.)**0.5\n",
    "\n",
    "    # We compute the test log-likelihood\n",
    "    ll = (logsumexp(-0.5 * tau * (testing_set.data.numpy() - Yt_hat)**2., 0) - np.log(T)\n",
    "        - 0.5*np.log(2*np.pi) + 0.5*np.log(tau))\n",
    "    test_ll = np.mean(ll)\n",
    "\n",
    "    # We are done!\n",
    "    print(rmse_standard_pred)\n",
    "    print(rmse)\n",
    "    print(test_ll)\n",
    "    \n",
    "    return rmse_standard_pred, rmse, test_ll\n",
    "\n",
    "prediction(network,testing_set,normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI - Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tau = 0.005 ; dropout_rate = 0.1\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Using a target size (torch.Size([71])) that is different to the input size (torch.Size([71, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([50, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.452843786471128\n",
      "23.190640054401527\n",
      "-28.96031\n",
      "tau = 0.005 ; dropout_rate = 0.15\n",
      "Training\n",
      "29.344012555421838\n",
      "21.684931094478156\n",
      "-37.133213\n",
      "tau = 0.005 ; dropout_rate = 0.2\n",
      "Training\n",
      "30.448301180775772\n",
      "21.92618592903315\n",
      "-49.797543\n",
      "tau = 0.01 ; dropout_rate = 0.1\n",
      "Training\n",
      "31.545179217439365\n",
      "23.93256058291063\n",
      "-30.708101\n",
      "tau = 0.01 ; dropout_rate = 0.15\n",
      "Training\n",
      "29.400744839142693\n",
      "21.804371890630602\n",
      "-37.523056\n",
      "tau = 0.01 ; dropout_rate = 0.2\n",
      "Training\n",
      "31.545204062692633\n",
      "22.915098638353463\n",
      "-54.229984\n",
      "tau = 0.05 ; dropout_rate = 0.1\n",
      "Training\n",
      "31.248834670758256\n",
      "25.625427540259572\n",
      "-34.902348\n",
      "tau = 0.05 ; dropout_rate = 0.15\n",
      "Training\n",
      "30.1684748879866\n",
      "24.09820808301937\n",
      "-45.419292\n",
      "tau = 0.05 ; dropout_rate = 0.2\n",
      "Training\n",
      "30.60469858194638\n",
      "21.031942869112434\n",
      "-45.95659\n",
      "tau = 0.1 ; dropout_rate = 0.1\n",
      "Training\n",
      "30.21254818029489\n",
      "22.226585841376146\n",
      "-26.770727\n",
      "tau = 0.1 ; dropout_rate = 0.15\n",
      "Training\n",
      "28.744727977954348\n",
      "21.632440775291272\n",
      "-36.963036\n",
      "tau = 0.1 ; dropout_rate = 0.2\n",
      "Training\n",
      "30.874746460647536\n",
      "21.31038210013857\n",
      "-47.13505\n"
     ]
    }
   ],
   "source": [
    "for dropout_rate in [0.005, 0.01, 0.05, 0.1]:\n",
    "    for tau in [0.1, 0.15, 0.2]:\n",
    "        print(\"tau = {} ; dropout_rate = {}\".format(dropout_rate,tau))\n",
    "        # Set the parameters\n",
    "        N=training_set.data.size()[0]\n",
    "        n_in=training_set.data.size()[1]\n",
    "        n_out=1\n",
    "        layer_sizes=[100,]\n",
    "        l=1e-2\n",
    "        lbda = (dropout_rate*l**2)/(2*N*tau)\n",
    "\n",
    "        # Create the network\n",
    "        network = Net(N=N,\n",
    "                      n_in=n_in,\n",
    "                      n_out=n_out,\n",
    "                      layer_sizes=[256],\n",
    "                      dropout_rate=0.5,\n",
    "                      tau=0.5).float()\n",
    "\n",
    "        # Type of the problem\n",
    "        problem=\"regression\"\n",
    "\n",
    "        # Training parameters\n",
    "        n_epochs = 100\n",
    "        learning_rate = 1e-8\n",
    "        momentum = 0.5\n",
    "\n",
    "        optimizer = optim.Adam(network.parameters(), \n",
    "                              lr=learning_rate,\n",
    "                             weight_decay=lbda)\n",
    "\n",
    "        train_losses = []\n",
    "        train_counter = []\n",
    "        test_losses = []\n",
    "        test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "        \n",
    "        print(\"Training\")\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train(network,epoch)\n",
    "            test(network)\n",
    "    \n",
    "        prediction(network, testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V - For MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result=np.zeros((1,10))\n",
    "\n",
    "#network.train()\n",
    "\n",
    "#for i in range(100):\n",
    "#    output=network(training_set.data[0].float())\n",
    "#    if problem==\"classification\":\n",
    "#        loss = F.nll_loss(output.unsqueeze(0), training_set.targets[0].unsqueeze(dim=0))\n",
    "#    loss.backward()\n",
    "#    pred = output.argmax()\n",
    "#    print(pred,training_set.targets[0])\n",
    "#    result=np.concatenate((result,[output.data.numpy()]),axis=0)\n",
    "    \n",
    "#result=result[1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
